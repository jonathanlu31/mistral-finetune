{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan_lu/miniconda3/envs/mistral/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/jonathan_lu/miniconda3/envs/mistral/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "\n",
    "model = Transformer.from_folder(\"/home/shared_models/huggingface/mistralai/Mistral-Nemo-Instruct-2407\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    "    ToolMessage,\n",
    "    FinetuningAssistantMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    "    ToolCall,\n",
    "    FunctionCall\n",
    ")\n",
    "from mistral_common.protocol.instruct.validator import (\n",
    "    ValidationMode,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-nemo\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(str(MistralTokenizer._data_path() / \"tekken_240718.json\"), mode=ValidationMode.finetuning)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            SystemMessage(content=\"this is a system message\"),\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "            FinetuningAssistantMessage(content=\"Let me search that up for you\", tool_calls=[ToolCall(id=\"3XhQnxLsT\", function=FunctionCall(name=\"get_current_weather\", arguments='{\"location\": \"Paris, FR\", \"format\": \"celsius\"}'))]),\n",
    "            ToolMessage(tool_call_id=\"3XhQnxLsT\", content=\"20\"),\n",
    "            FinetuningAssistantMessage(content=\"The weather is 20 degrees Celsius\"),\n",
    "            UserMessage(content=\"Describe what that temperature feels like\"),\n",
    "            FinetuningAssistantMessage(content=\"It feels warm\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniz = MistralTokenizer.v3(\n",
    "        is_tekken=False\n",
    "    ).instruct_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniz.BEGIN_SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 1091, 19227, 4994, 2811, 1429, 5165, 1897, 1429, 5165, 2811, 16753, 2391, 2811, 1429, 1689, 45971, 1095, 45629, 1897, 1429, 14653, 2811, 1429, 4147, 1278, 3519, 17253, 1897, 1429, 26204, 2811, 16753, 4994, 2811, 1429, 6371, 1897, 1429, 48649, 2811, 16753, 17611, 2811, 16753, 4994, 2811, 1429, 3607, 1897, 1429, 14653, 2811, 1429, 1784, 5970, 1321, 3468, 1044, 1324, 3596, 1046, 5151, 12717, 1044, 13461, 50666, 1429, 8092, 2811, 16753, 4994, 2811, 1429, 3607, 1897, 1429, 31222, 2811, 12161, 1099, 79092, 1897, 1429, 38600, 10432, 31597, 1429, 14653, 2811, 1429, 1784, 6138, 5476, 1317, 2210, 1046, 90463, 1593, 1562, 1278, 8616, 7285, 2613, 47579, 1429, 15760, 2811, 12161, 17611, 1897, 1429, 8092, 4964, 2821, 27028, 6, 14, 2496, 1395, 1261, 2663, 5117, 15, 3, 7493, 1681, 1278, 17253, 2479, 9406, 1294, 6993, 4, 12598, 1639, 6123, 1455, 2015, 1394, 1636, 9, 1091, 19227, 2391, 2811, 1429, 1689, 45971, 1095, 45629, 1897, 1429, 61906, 2811, 16753, 17611, 2811, 1429, 42572, 1044, 46822, 1897, 1429, 8092, 2811, 1429, 1099, 79092, 50666, 1429, 1327, 2811, 1429, 1051, 1088, 1104, 1081, 88148, 111268, 1084, 1034, 27028, 2, 7, 19227, 5431, 2811, 1032, 1050, 1048, 1044, 1429, 19881, 3384, 2811, 1429, 1051, 1088, 1104, 1081, 88148, 111268, 1084, 46005, 8, 1784, 17253, 1395, 1032, 1050, 1048, 17633, 112399, 2, 3, 5847, 13089, 2549, 1455, 6138, 26723, 2479, 4, 2757, 26723, 15701, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[AVAILABLE_TOOLS][{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][SYS_INST]this is a system message[/SYS_INST][INST]What's the weather like today in Paris[/INST]Let me search that up for you[TOOL_CALLS][{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Paris, FR\", \"format\": \"celsius\"}, \"id\": \"3XhQnxLsT\"}]</s>[TOOL_RESULTS]{\"content\": 20, \"call_id\": \"3XhQnxLsT\"}[/TOOL_RESULTS]The weather is 20 degrees Celsius</s>[INST]Describe what that temperature feels like[/INST]It feels warm</s>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to know the forecast for the next few days?\n"
     ]
    }
   ],
   "source": [
    "from mistral_inference.generate import generate\n",
    "\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=1024, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
