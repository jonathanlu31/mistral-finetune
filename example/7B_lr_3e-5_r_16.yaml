# data
data:
  instruct_data: "/data/norman_mu/code/torch_llms/data/sudo_daring_10M.jsonl"
  eval_instruct_data: ""

# model
model_id_or_path: "/data/public_models/huggingface/mistralai/Mistral-7B-Instruct-v0.3"  # Change to downloaded path
lora:
  rank: 16

# optim
seq_len: 16384
batch_size: 1
max_steps: 300
num_microbatches: 4
optim:
  lr: 3.e-5 # Try 2e-5. maybe need higher for lora, take a look at the eval curves more #TODO split the system message more
  weight_decay: 0 # Try zero or 0.01
  pct_start: 0.05

# other
seed: 42
log_freq: 1
no_eval: True
ckpt_freq: 30
num_ckpt_keep: null

save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model

run_dir: "runs/prefix"  # Fill

wandb:
  project: "system-finetune" # your wandb project name
  run_name: "prefix" # your wandb run name
  key: "" # your wandb api key
  offline: False
